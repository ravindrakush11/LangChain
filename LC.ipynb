{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd4dSi9WSye6gONB06hIZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrakush11/LangChain/blob/main/LC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq uvicorn lida kaleido python-multipart"
      ],
      "metadata": {
        "id": "kp7IIJG7hmm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESnquG9RhFur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34d9f0fc-dc9e-49b1-c8b3-f646e6abf454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.9/493.9 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq langchain langchain-cli langsmith"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq \"langserve[all]\""
      ],
      "metadata": {
        "id": "VPjRGE7IoH3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq cohere tiktoken openai"
      ],
      "metadata": {
        "id": "iL2MVCe7hXu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY= \" \""
      ],
      "metadata": {
        "id": "uVT-kiimiPTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM/ChatModel"
      ],
      "metadata": {
        "id": "pGS3SQAJvnbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key = 'OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Et7mav5tmAOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(openai_api_key = OPENAI_API_KEY)\n",
        "chat_model = ChatOpenAI(openai_api_key = 'OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "4fUxbRrrsMB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "text = \"Provide a recipe of cake for 5 year old B'day baby\"\n",
        "messages = [HumanMessage(context = text)]"
      ],
      "metadata": {
        "id": "Fx_KwU1asRIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt Template"
      ],
      "metadata": {
        "id": "eKV9W0SLvs10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"Provide a recipe of cake for {year} year old B'day {person}\")\n",
        "p = prompt.format(year = \"5\",\n",
        "                  person = \"baby\")\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rdWKZFSbuS9S",
        "outputId": "756f4a0e-3833-4e07-81c4-0ddb39695b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Provide a recipe of cake for 5 year old B'day baby\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing a list of messages\n",
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "template = \"What are the present that I can give to  my {relation} for a {day}\"\n",
        "\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template)\n",
        "])\n",
        "\n",
        "c = chat_prompt.format_messages(relation = 'girl friend', day = 'valentine day', text = 'I like comedy')\n",
        "c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe0CIvEoy7cE",
        "outputId": "a85ffe35-7184-4e46-f333-d0505317aa94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='What are the present that I can give to  my girl friend for a valentine day'),\n",
              " HumanMessage(content='I like comedy')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output Parsers"
      ],
      "metadata": {
        "id": "2JPF57S116JZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "  \"\"\"Parse the output of an LLM call (generated message) to a comma separated list\"\"\"\n",
        "\n",
        "\n",
        "  def parse(self, text:str):\n",
        "    \"\"\"Parse the output of an LLM call\"\"\"\n",
        "    return text.strip().split(\", \")\n",
        "\n",
        "CommaSeparatedListOutputParser().parse(\"subject, verb, object, predicate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNwxXHro19jY",
        "outputId": "f6949e94-4957-4b73-f86d-0d36293997d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['subject', 'verb', 'object', 'predicate']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Composing with LCEL"
      ],
      "metadata": {
        "id": "BPbo_tZT6yJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "  \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "\n",
        "  def parse(self, text:str):\n",
        "    \"\"\"Parse the output of an LLM call\"\"\"\n",
        "    return text.strip().split(\", \")\n",
        "\n",
        "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
        "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
        "ONLY return a comma separated list, and nothing more.\"\"\"\n",
        "\n",
        "\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "\n",
        "chain = chat_prompt | ChatOpenAI | CommaSeparatedListOutputParser()\n",
        "chain.invoke({\"text\": \"color\"})"
      ],
      "metadata": {
        "id": "sHs8Qeh161jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8J50QbxmkRbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangServe"
      ],
      "metadata": {
        "id": "5p1kOCCgwCAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq langserve"
      ],
      "metadata": {
        "id": "906WN-1BwaOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from langserve import add_routes\n",
        "\n",
        "#2. App definition\n",
        "app = FastAPI(\n",
        "    title = \"LangChain Server\",\n",
        "    version = \"1.0\",\n",
        "    description = \"A simple api server using Langchain's Runnable interfaces\",\n",
        ")\n",
        "\n",
        "#3. Adding chain route\n",
        "add_route(\n",
        "    app,\n",
        "    category_chain,\n",
        "    path = '/category_chain'\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  import uvicorn\n",
        "  uvicorn.run(app, host = 'localhost', port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "7jn4r_oCwJUM",
        "outputId": "3d90d0d6-a642-4d4e-cebe-60aa7e3fd2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0c5f1bf81475>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#3. Adding chain route\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m add_route(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mapp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mcategory_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'add_route' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Client"
      ],
      "metadata": {
        "id": "n1mHnSoIKVEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langserve import RemoteRunnable\n",
        "remote_chain = RemoteRunnable(\"http://localhost:8000/category_chain/\")\n",
        "remote_chain.invoke({\"text\": \"colors\"})"
      ],
      "metadata": {
        "id": "wqi1KWa_KSmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}