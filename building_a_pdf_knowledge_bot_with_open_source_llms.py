# -*- coding: utf-8 -*-
"""Building a PDF Knowledge Bot With Open-Source LLMs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gGHcPpJNNTIx5kHy2DgRzGneS_SrXU-a
"""

!pip install langchain
!pip install chromadb
!pip install pdfplumber

!pip install tiktoken
!pip install lxml
!pip install torch
!pip install transformers
!pip install accelerate
!pip install sentence-transformers
!pip install einops
!pip install xformers

from langchain.document_loaders import PDFPlumberLoader,  TextLoader
from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter
from transformers import pipeline
from langchain.prompts import PromptTemplate

from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain import HuggingFacePipeline
from langchain.embeddings import HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings

import torch
from transformers import AutoTokenizer
import re
import os

from sentence_transformers import SentenceTransformer

from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings

#Embedding model
EMB_SBERT_MPNET_BASE = "sentence-transformers/all-mpnet-base-v2"
model = SentenceTransformer(EMB_SBERT_MPNET_BASE)

LLM_FLAN_T5_BASE = "google/flan-t5-base"

config = {"persist_directory":None,
          "load_in_8bit":False,
          "embedding" : EMB_SBERT_MPNET_BASE,
          "llm":LLM_FLAN_T5_BASE,
          }

def create_sbert_mpnet():
        device = "cuda" if torch.cuda.is_available() else "cpu"
        return HuggingFaceEmbeddings(model_name=EMB_SBERT_MPNET_BASE, model_kwargs={"device": device})


def create_flan_t5_base(load_in_8bit=False):
        # Wrap it in HF pipeline for use with LangChain
        model="google/flan-t5-base"
        tokenizer = AutoTokenizer.from_pretrained(model)
        return pipeline(
            task="text2text-generation",
            model=model,
            tokenizer = tokenizer,
            max_new_tokens=100,
            model_kwargs={"device_map": "auto", "load_in_8bit": load_in_8bit, "max_length": 512, "temperature": 0.5}
        )



if config["embedding"] == EMB_SBERT_MPNET_BASE:
    embedding = create_sbert_mpnet()
load_in_8bit = config["load_in_8bit"]
if config["llm"] == LLM_FLAN_T5_BASE:
    llm = create_flan_t5_base(load_in_8bit=load_in_8bit)

# Load the pdf
pdf_path = "/content/The 2011 Cricket World Cup.pdf"
loader = PDFPlumberLoader(pdf_path)
documents = loader.load()

# If you can't find the directory where your index/collection is stored in order to remove it,
# Removed the invalid dimensionality error

from chromadb.errors import InvalidDimensionException
try:
    docsearch = Chroma.from_documents(documents = documents, embedding=embedding)
except InvalidDimensionException:
    Chroma().delete_collection()
    docsearch = Chroma.from_documents(documents=documents, embedding=embedding)

# Split documents and create text snippets
text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)
texts = text_splitter.split_documents(texts)

persist_directory = config["persist_directory"]
vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)

hf_llm = HuggingFacePipeline(pipeline=llm)
retriever = vectordb.as_retriever(search_kwargs={"k":4})
qa = RetrievalQA.from_chain_type(llm=hf_llm, chain_type="stuff",retriever=retriever)

# Defining a default prompt for flan models
if  config["llm"] == LLM_FLAN_T5_BASE:
    question_t5_template = """
    context: {context}
    question: {question}
    answer:
    """
    QUESTION_T5_PROMPT = PromptTemplate(
        template=question_t5_template, input_variables=["context", "question"]
    )
    qa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT

question = input("")
qa.combine_documents_chain.verbose = True
qa.return_source_documents = True
res  = qa({"query":question,})
print(f"Answer: '{res['result']}'")

# Assume you have a Chroma instance `chroma_instance` and the source document `source_doc`
ids_to_delete = []

for doc in chroma_instance:
    if doc.metadata.get('source') == source_doc:
        ids_to_delete.append(doc.id)

chroma_instance.delete(ids=ids_to_delete)